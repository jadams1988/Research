{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-10 15:53:35,580] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "#Get the environment and extract the number of actions available in the Cartpole problem\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 114\n",
      "Trainable params: 114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n",
      "   10/5000: episode: 1, duration: 0.212s, episode steps: 10, steps per second: 47, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.132 [-1.967, 3.014], loss: --, mean_absolute_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joey\\Anaconda3\\lib\\site-packages\\keras_rl-0.3.0-py3.6.egg\\rl\\memory.py:29: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   19/5000: episode: 2, duration: 0.768s, episode steps: 9, steps per second: 12, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.161 [-1.326, 2.303], loss: 0.504959, mean_absolute_error: 0.616374, mean_q: 0.271113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joey\\Anaconda3\\lib\\site-packages\\keras_rl-0.3.0-py3.6.egg\\rl\\memory.py:29: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   33/5000: episode: 3, duration: 0.234s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.125 [-1.517, 2.553], loss: 0.436633, mean_absolute_error: 0.580476, mean_q: 0.333729\n",
      "   42/5000: episode: 4, duration: 0.152s, episode steps: 9, steps per second: 59, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.137 [-1.783, 2.784], loss: 0.405209, mean_absolute_error: 0.571296, mean_q: 0.396374\n",
      "   53/5000: episode: 5, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.148 [-1.915, 2.968], loss: 0.374215, mean_absolute_error: 0.557008, mean_q: 0.457941\n",
      "   65/5000: episode: 6, duration: 0.200s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.129 [-1.559, 2.544], loss: 0.338577, mean_absolute_error: 0.560679, mean_q: 0.563106\n",
      "   75/5000: episode: 7, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.155 [-1.913, 3.101], loss: 0.322740, mean_absolute_error: 0.551483, mean_q: 0.613148\n",
      "   84/5000: episode: 8, duration: 0.158s, episode steps: 9, steps per second: 57, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.139 [-1.763, 2.739], loss: 0.293230, mean_absolute_error: 0.556158, mean_q: 0.733560\n",
      "   93/5000: episode: 9, duration: 0.151s, episode steps: 9, steps per second: 59, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.159 [-1.716, 2.815], loss: 0.283948, mean_absolute_error: 0.567003, mean_q: 0.803239\n",
      "  102/5000: episode: 10, duration: 0.147s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.146 [-1.583, 2.546], loss: 0.264012, mean_absolute_error: 0.563452, mean_q: 0.873386\n",
      "  111/5000: episode: 11, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.134 [-1.787, 2.754], loss: 0.258983, mean_absolute_error: 0.578522, mean_q: 0.927410\n",
      "  121/5000: episode: 12, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.158 [-1.915, 3.108], loss: 0.252228, mean_absolute_error: 0.574854, mean_q: 0.976529\n",
      "  131/5000: episode: 13, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.150 [-1.912, 3.013], loss: 0.254505, mean_absolute_error: 0.597345, mean_q: 1.068637\n",
      "  141/5000: episode: 14, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.151 [-1.939, 3.082], loss: 0.240145, mean_absolute_error: 0.595221, mean_q: 1.126133\n",
      "  152/5000: episode: 15, duration: 0.181s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.126 [-1.770, 2.886], loss: 0.260917, mean_absolute_error: 0.625524, mean_q: 1.213981\n",
      "  161/5000: episode: 16, duration: 0.154s, episode steps: 9, steps per second: 59, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.132 [-1.365, 2.274], loss: 0.224576, mean_absolute_error: 0.621988, mean_q: 1.273582\n",
      "  172/5000: episode: 17, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.125 [-1.744, 2.781], loss: 0.241041, mean_absolute_error: 0.647492, mean_q: 1.358936\n",
      "  182/5000: episode: 18, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [-1.965, 3.056], loss: 0.251282, mean_absolute_error: 0.674547, mean_q: 1.472741\n",
      "  192/5000: episode: 19, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.173 [-1.932, 3.121], loss: 0.257185, mean_absolute_error: 0.705309, mean_q: 1.597362\n",
      "  201/5000: episode: 20, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.153 [-1.756, 2.807], loss: 0.254745, mean_absolute_error: 0.741785, mean_q: 1.690752\n",
      "  209/5000: episode: 21, duration: 0.130s, episode steps: 8, steps per second: 61, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.169 [-1.546, 2.557], loss: 0.251192, mean_absolute_error: 0.762287, mean_q: 1.700664\n",
      "  218/5000: episode: 22, duration: 0.152s, episode steps: 9, steps per second: 59, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.145 [-1.773, 2.813], loss: 0.250064, mean_absolute_error: 0.805718, mean_q: 1.827709\n",
      "  226/5000: episode: 23, duration: 0.130s, episode steps: 8, steps per second: 61, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.157 [-1.531, 2.553], loss: 0.245657, mean_absolute_error: 0.812118, mean_q: 1.850366\n",
      "  237/5000: episode: 24, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.129 [-1.359, 2.269], loss: 0.285642, mean_absolute_error: 0.847797, mean_q: 1.917416\n",
      "  247/5000: episode: 25, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.116 [-1.773, 2.692], loss: 0.312384, mean_absolute_error: 0.886675, mean_q: 1.907272\n",
      "  255/5000: episode: 26, duration: 0.133s, episode steps: 8, steps per second: 60, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-1.551, 2.549], loss: 0.266804, mean_absolute_error: 0.902080, mean_q: 1.917899\n",
      "  265/5000: episode: 27, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.118 [-1.964, 3.004], loss: 0.281839, mean_absolute_error: 0.941554, mean_q: 1.990349\n",
      "  275/5000: episode: 28, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.118 [-1.616, 2.601], loss: 0.220668, mean_absolute_error: 0.924871, mean_q: 2.046453\n",
      "  285/5000: episode: 29, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.175 [-1.910, 3.113], loss: 0.288559, mean_absolute_error: 0.994467, mean_q: 2.184567\n",
      "  294/5000: episode: 30, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [-1.717, 2.796], loss: 0.267899, mean_absolute_error: 1.006461, mean_q: 2.249717\n",
      "  304/5000: episode: 31, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.127 [-1.985, 3.032], loss: 0.274018, mean_absolute_error: 1.016625, mean_q: 2.332478\n",
      "  313/5000: episode: 32, duration: 0.147s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [-1.807, 2.880], loss: 0.284410, mean_absolute_error: 1.024878, mean_q: 2.378659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  322/5000: episode: 33, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.145 [-1.737, 2.764], loss: 0.340623, mean_absolute_error: 1.074081, mean_q: 2.470546\n",
      "  331/5000: episode: 34, duration: 0.154s, episode steps: 9, steps per second: 58, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.156 [-1.712, 2.836], loss: 0.314549, mean_absolute_error: 1.073343, mean_q: 2.458197\n",
      "  339/5000: episode: 35, duration: 0.132s, episode steps: 8, steps per second: 61, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.144 [-1.590, 2.565], loss: 0.334980, mean_absolute_error: 1.093307, mean_q: 2.421076\n",
      "  350/5000: episode: 36, duration: 0.180s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.116 [-1.783, 2.796], loss: 0.324130, mean_absolute_error: 1.145594, mean_q: 2.493261\n",
      "  360/5000: episode: 37, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.135 [-1.937, 3.009], loss: 0.299405, mean_absolute_error: 1.133754, mean_q: 2.528645\n",
      "  368/5000: episode: 38, duration: 0.136s, episode steps: 8, steps per second: 59, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-1.547, 2.531], loss: 0.290274, mean_absolute_error: 1.156844, mean_q: 2.604302\n",
      "  378/5000: episode: 39, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.137 [-1.958, 3.050], loss: 0.256382, mean_absolute_error: 1.157918, mean_q: 2.650755\n",
      "  387/5000: episode: 40, duration: 0.155s, episode steps: 9, steps per second: 58, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.144 [-1.774, 2.819], loss: 0.301516, mean_absolute_error: 1.209974, mean_q: 2.744029\n",
      "  395/5000: episode: 41, duration: 0.129s, episode steps: 8, steps per second: 62, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.163 [-1.368, 2.236], loss: 0.279091, mean_absolute_error: 1.228021, mean_q: 2.802107\n",
      "  404/5000: episode: 42, duration: 0.154s, episode steps: 9, steps per second: 58, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.135 [-1.809, 2.812], loss: 0.318836, mean_absolute_error: 1.304744, mean_q: 2.880626\n",
      "  412/5000: episode: 43, duration: 0.126s, episode steps: 8, steps per second: 63, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.174 [-1.530, 2.589], loss: 0.363163, mean_absolute_error: 1.301765, mean_q: 2.893583\n",
      "  424/5000: episode: 44, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.136 [-1.915, 3.073], loss: 0.412879, mean_absolute_error: 1.366030, mean_q: 2.881391\n",
      "  433/5000: episode: 45, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.167 [-1.721, 2.843], loss: 0.325620, mean_absolute_error: 1.344390, mean_q: 2.862868\n",
      "  442/5000: episode: 46, duration: 0.155s, episode steps: 9, steps per second: 58, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [-1.727, 2.899], loss: 0.332203, mean_absolute_error: 1.350159, mean_q: 2.965731\n",
      "  451/5000: episode: 47, duration: 0.145s, episode steps: 9, steps per second: 62, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.164 [-1.721, 2.789], loss: 0.332399, mean_absolute_error: 1.372930, mean_q: 3.071724\n",
      "  460/5000: episode: 48, duration: 0.157s, episode steps: 9, steps per second: 57, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.124 [-1.794, 2.794], loss: 0.434496, mean_absolute_error: 1.434607, mean_q: 3.130515\n",
      "  469/5000: episode: 49, duration: 0.143s, episode steps: 9, steps per second: 63, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-1.768, 2.798], loss: 0.352185, mean_absolute_error: 1.422645, mean_q: 3.090333\n",
      "  481/5000: episode: 50, duration: 0.200s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.091 [-1.975, 3.007], loss: 0.319767, mean_absolute_error: 1.412736, mean_q: 3.194272\n",
      "  493/5000: episode: 51, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.113 [-1.533, 2.532], loss: 0.280193, mean_absolute_error: 1.421557, mean_q: 3.315490\n",
      "  503/5000: episode: 52, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.124 [-1.986, 3.002], loss: 0.393323, mean_absolute_error: 1.503114, mean_q: 3.406795\n",
      "  515/5000: episode: 53, duration: 0.195s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.107 [-1.993, 3.002], loss: 0.318292, mean_absolute_error: 1.491979, mean_q: 3.433394\n",
      "  524/5000: episode: 54, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.135 [-1.765, 2.769], loss: 0.351407, mean_absolute_error: 1.494563, mean_q: 3.508605\n",
      "  533/5000: episode: 55, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.145 [-1.742, 2.813], loss: 0.314141, mean_absolute_error: 1.513596, mean_q: 3.550789\n",
      "  549/5000: episode: 56, duration: 0.273s, episode steps: 16, steps per second: 59, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.064 [-1.595, 2.487], loss: 0.380151, mean_absolute_error: 1.566396, mean_q: 3.538563\n",
      "  558/5000: episode: 57, duration: 0.144s, episode steps: 9, steps per second: 63, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-1.775, 2.809], loss: 0.276464, mean_absolute_error: 1.553269, mean_q: 3.603781\n",
      "  572/5000: episode: 58, duration: 0.231s, episode steps: 14, steps per second: 61, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.143 [0.000, 1.000], mean observation: 0.090 [-1.924, 3.025], loss: 0.358269, mean_absolute_error: 1.635726, mean_q: 3.623549\n",
      "  580/5000: episode: 59, duration: 0.133s, episode steps: 8, steps per second: 60, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.140 [-1.605, 2.571], loss: 0.260972, mean_absolute_error: 1.627634, mean_q: 3.661498\n",
      "  590/5000: episode: 60, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.149 [-1.997, 3.097], loss: 0.300571, mean_absolute_error: 1.672382, mean_q: 3.784444\n",
      "  599/5000: episode: 61, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.127 [-1.773, 2.767], loss: 0.270193, mean_absolute_error: 1.700701, mean_q: 3.805477\n",
      "  609/5000: episode: 62, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.134 [-1.950, 3.076], loss: 0.194491, mean_absolute_error: 1.684996, mean_q: 3.933551\n",
      "  620/5000: episode: 63, duration: 0.179s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.130 [-1.718, 2.699], loss: 0.308571, mean_absolute_error: 1.782853, mean_q: 3.942419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  630/5000: episode: 64, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.135 [-1.523, 2.481], loss: 0.289703, mean_absolute_error: 1.816219, mean_q: 3.848078\n",
      "  641/5000: episode: 65, duration: 0.188s, episode steps: 11, steps per second: 58, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.118 [-1.578, 2.436], loss: 0.255801, mean_absolute_error: 1.835965, mean_q: 3.895579\n",
      "  651/5000: episode: 66, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.132 [-1.548, 2.481], loss: 0.231920, mean_absolute_error: 1.842907, mean_q: 3.988597\n",
      "  661/5000: episode: 67, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.117 [-1.568, 2.437], loss: 0.268754, mean_absolute_error: 1.862454, mean_q: 4.027606\n",
      "  670/5000: episode: 68, duration: 0.151s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.142 [-1.351, 2.262], loss: 0.287224, mean_absolute_error: 1.894011, mean_q: 4.078995\n",
      "  683/5000: episode: 69, duration: 0.216s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.083 [-1.550, 2.403], loss: 0.268140, mean_absolute_error: 1.916205, mean_q: 4.050002\n",
      "  693/5000: episode: 70, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.149 [-1.551, 2.505], loss: 0.312550, mean_absolute_error: 1.986164, mean_q: 4.190695\n",
      "  701/5000: episode: 71, duration: 0.134s, episode steps: 8, steps per second: 60, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.137 [-1.415, 2.186], loss: 0.291086, mean_absolute_error: 1.985731, mean_q: 4.107934\n",
      "  709/5000: episode: 72, duration: 0.132s, episode steps: 8, steps per second: 60, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.151 [-1.341, 2.229], loss: 0.295918, mean_absolute_error: 2.015465, mean_q: 4.060124\n",
      "  719/5000: episode: 73, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.113 [-1.414, 2.189], loss: 0.257674, mean_absolute_error: 2.020427, mean_q: 4.232862\n",
      "  727/5000: episode: 74, duration: 0.140s, episode steps: 8, steps per second: 57, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.157 [-1.336, 2.211], loss: 0.256352, mean_absolute_error: 2.043460, mean_q: 4.314719\n",
      "  736/5000: episode: 75, duration: 0.144s, episode steps: 9, steps per second: 63, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.143 [-1.544, 2.451], loss: 0.249776, mean_absolute_error: 2.061416, mean_q: 4.438872\n",
      "  746/5000: episode: 76, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.148 [-1.328, 2.205], loss: 0.197432, mean_absolute_error: 2.050790, mean_q: 4.456550\n",
      "  756/5000: episode: 77, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.120 [-1.733, 2.636], loss: 0.262108, mean_absolute_error: 2.098068, mean_q: 4.484810\n",
      "  769/5000: episode: 78, duration: 0.226s, episode steps: 13, steps per second: 58, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.116 [-1.332, 2.292], loss: 0.257253, mean_absolute_error: 2.126979, mean_q: 4.518392\n",
      "  779/5000: episode: 79, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.143 [-1.336, 2.212], loss: 0.221585, mean_absolute_error: 2.127630, mean_q: 4.602332\n",
      "  789/5000: episode: 80, duration: 0.163s, episode steps: 10, steps per second: 62, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.139 [-1.522, 2.431], loss: 0.304281, mean_absolute_error: 2.158872, mean_q: 4.495317\n",
      "  799/5000: episode: 81, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.149 [-1.358, 2.226], loss: 0.258819, mean_absolute_error: 2.182098, mean_q: 4.580124\n",
      "  809/5000: episode: 82, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.128 [-1.520, 2.382], loss: 0.254914, mean_absolute_error: 2.168018, mean_q: 4.596259\n",
      "  819/5000: episode: 83, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.136 [-1.589, 2.522], loss: 0.246363, mean_absolute_error: 2.181217, mean_q: 4.641424\n",
      "  830/5000: episode: 84, duration: 0.191s, episode steps: 11, steps per second: 58, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.133 [-1.522, 2.484], loss: 0.270839, mean_absolute_error: 2.231720, mean_q: 4.737736\n",
      "  839/5000: episode: 85, duration: 0.142s, episode steps: 9, steps per second: 64, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.147 [-1.575, 2.470], loss: 0.250444, mean_absolute_error: 2.224379, mean_q: 4.680868\n",
      "  848/5000: episode: 86, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.134 [-1.554, 2.426], loss: 0.249211, mean_absolute_error: 2.256808, mean_q: 4.749856\n",
      "  858/5000: episode: 87, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.132 [-1.515, 2.363], loss: 0.249442, mean_absolute_error: 2.289698, mean_q: 4.895220\n",
      "  871/5000: episode: 88, duration: 0.210s, episode steps: 13, steps per second: 62, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.087 [-1.594, 2.317], loss: 0.246615, mean_absolute_error: 2.339462, mean_q: 4.887253\n",
      "  881/5000: episode: 89, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.139 [-1.588, 2.417], loss: 0.213594, mean_absolute_error: 2.395696, mean_q: 4.944264\n",
      "  892/5000: episode: 90, duration: 0.181s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.106 [-1.327, 2.075], loss: 0.281896, mean_absolute_error: 2.415138, mean_q: 4.897078\n",
      "  905/5000: episode: 91, duration: 0.217s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.089 [-1.510, 2.284], loss: 0.212253, mean_absolute_error: 2.402803, mean_q: 4.948731\n",
      "  915/5000: episode: 92, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.137 [-1.781, 2.704], loss: 0.274344, mean_absolute_error: 2.422761, mean_q: 4.933572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  925/5000: episode: 93, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.113 [-1.564, 2.350], loss: 0.241356, mean_absolute_error: 2.470480, mean_q: 5.040570\n",
      "  935/5000: episode: 94, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.128 [-1.585, 2.415], loss: 0.227599, mean_absolute_error: 2.482512, mean_q: 5.075848\n",
      "  943/5000: episode: 95, duration: 0.133s, episode steps: 8, steps per second: 60, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.160 [-1.142, 2.028], loss: 0.213274, mean_absolute_error: 2.459292, mean_q: 5.005486\n",
      "  952/5000: episode: 96, duration: 0.152s, episode steps: 9, steps per second: 59, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.139 [-1.517, 2.415], loss: 0.215590, mean_absolute_error: 2.517670, mean_q: 5.081820\n",
      "  964/5000: episode: 97, duration: 0.200s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.104 [-1.577, 2.393], loss: 0.216744, mean_absolute_error: 2.536607, mean_q: 5.101368\n",
      "  972/5000: episode: 98, duration: 0.132s, episode steps: 8, steps per second: 61, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.145 [-1.343, 2.226], loss: 0.223879, mean_absolute_error: 2.566924, mean_q: 5.163010\n",
      "  981/5000: episode: 99, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.130 [-1.383, 2.265], loss: 0.210429, mean_absolute_error: 2.549308, mean_q: 5.091454\n",
      "  991/5000: episode: 100, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.097 [-1.603, 2.335], loss: 0.166828, mean_absolute_error: 2.533511, mean_q: 5.088809\n",
      " 1001/5000: episode: 101, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.112 [-1.554, 2.351], loss: 0.192351, mean_absolute_error: 2.547860, mean_q: 5.073632\n",
      " 1014/5000: episode: 102, duration: 0.218s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.100 [-1.768, 2.698], loss: 0.149619, mean_absolute_error: 2.622189, mean_q: 5.273581\n",
      " 1025/5000: episode: 103, duration: 0.173s, episode steps: 11, steps per second: 64, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.144 [-1.751, 2.765], loss: 0.198421, mean_absolute_error: 2.664200, mean_q: 5.253120\n",
      " 1033/5000: episode: 104, duration: 0.135s, episode steps: 8, steps per second: 59, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.137 [-1.565, 2.522], loss: 0.178697, mean_absolute_error: 2.779712, mean_q: 5.419886\n",
      " 1046/5000: episode: 105, duration: 0.220s, episode steps: 13, steps per second: 59, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.084 [-1.382, 2.011], loss: 0.161547, mean_absolute_error: 2.718402, mean_q: 5.285001\n",
      " 1057/5000: episode: 106, duration: 0.176s, episode steps: 11, steps per second: 62, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.110 [-1.373, 2.051], loss: 0.142824, mean_absolute_error: 2.799951, mean_q: 5.481601\n",
      " 1067/5000: episode: 107, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.148 [-1.548, 2.420], loss: 0.137732, mean_absolute_error: 2.692737, mean_q: 5.255431\n",
      " 1079/5000: episode: 108, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.140 [-1.327, 2.205], loss: 0.152273, mean_absolute_error: 2.768062, mean_q: 5.396789\n",
      " 1090/5000: episode: 109, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.130 [-3.285, 2.182], loss: 0.180878, mean_absolute_error: 2.798904, mean_q: 5.369033\n",
      " 1100/5000: episode: 110, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.158 [-2.765, 1.762], loss: 0.152157, mean_absolute_error: 2.856672, mean_q: 5.517838\n",
      " 1110/5000: episode: 111, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.151 [-2.541, 1.556], loss: 0.119176, mean_absolute_error: 2.868407, mean_q: 5.508634\n",
      " 1119/5000: episode: 112, duration: 0.157s, episode steps: 9, steps per second: 57, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.130 [-2.276, 1.393], loss: 0.443933, mean_absolute_error: 3.041629, mean_q: 5.728514\n",
      " 1129/5000: episode: 113, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.131 [-3.098, 1.981], loss: 0.401170, mean_absolute_error: 3.041222, mean_q: 5.790004\n",
      " 1139/5000: episode: 114, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.147 [-3.000, 1.919], loss: 0.520125, mean_absolute_error: 3.028720, mean_q: 5.813675\n",
      " 1149/5000: episode: 115, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.151 [-3.047, 1.919], loss: 1.560325, mean_absolute_error: 3.074580, mean_q: 5.714284\n",
      " 1160/5000: episode: 116, duration: 0.182s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.127 [-2.770, 1.732], loss: 1.144722, mean_absolute_error: 3.148904, mean_q: 5.877339\n",
      " 1169/5000: episode: 117, duration: 0.151s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.142 [-2.339, 1.384], loss: 0.645496, mean_absolute_error: 3.046641, mean_q: 5.646675\n",
      " 1177/5000: episode: 118, duration: 0.132s, episode steps: 8, steps per second: 61, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.148 [-2.520, 1.541], loss: 0.919466, mean_absolute_error: 3.316277, mean_q: 6.145689\n",
      " 1185/5000: episode: 119, duration: 0.131s, episode steps: 8, steps per second: 61, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.160 [-2.576, 1.578], loss: 0.911939, mean_absolute_error: 3.182409, mean_q: 5.901345\n",
      " 1193/5000: episode: 120, duration: 0.136s, episode steps: 8, steps per second: 59, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.147 [-2.572, 1.612], loss: 0.314273, mean_absolute_error: 3.110824, mean_q: 5.812612\n",
      " 1202/5000: episode: 121, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.173 [-2.848, 1.726], loss: 0.889791, mean_absolute_error: 3.202975, mean_q: 6.014709\n",
      " 1212/5000: episode: 122, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.125 [-3.027, 1.941], loss: 1.071274, mean_absolute_error: 3.385457, mean_q: 6.406307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1222/5000: episode: 123, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.162 [-2.614, 1.531], loss: 1.215921, mean_absolute_error: 3.270322, mean_q: 6.131730\n",
      " 1233/5000: episode: 124, duration: 0.185s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.142 [-3.306, 2.149], loss: 1.508476, mean_absolute_error: 3.302009, mean_q: 6.162461\n",
      " 1242/5000: episode: 125, duration: 0.149s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.138 [-2.783, 1.756], loss: 0.897659, mean_absolute_error: 3.392080, mean_q: 6.305478\n",
      " 1250/5000: episode: 126, duration: 0.130s, episode steps: 8, steps per second: 62, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.159 [-2.589, 1.596], loss: 2.440800, mean_absolute_error: 3.428294, mean_q: 6.129782\n",
      " 1263/5000: episode: 127, duration: 0.216s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.099 [-1.134, 1.682], loss: 0.980746, mean_absolute_error: 3.407430, mean_q: 6.267383\n",
      " 1289/5000: episode: 128, duration: 0.432s, episode steps: 26, steps per second: 60, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.808 [0.000, 1.000], mean observation: 0.053 [-3.869, 3.127], loss: 1.319342, mean_absolute_error: 3.411554, mean_q: 6.285251\n",
      " 1298/5000: episode: 129, duration: 0.153s, episode steps: 9, steps per second: 59, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.117 [-0.999, 1.605], loss: 1.088915, mean_absolute_error: 3.515679, mean_q: 6.551896\n",
      " 1310/5000: episode: 130, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.110 [-1.003, 1.613], loss: 0.857246, mean_absolute_error: 3.365993, mean_q: 6.310949\n",
      " 1319/5000: episode: 131, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.156 [-2.789, 1.742], loss: 2.669480, mean_absolute_error: 3.698586, mean_q: 6.658593\n",
      " 1329/5000: episode: 132, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.143 [-1.159, 1.849], loss: 2.598784, mean_absolute_error: 3.842322, mean_q: 6.951537\n",
      " 1343/5000: episode: 133, duration: 0.234s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.092 [-0.950, 1.576], loss: 1.314963, mean_absolute_error: 3.623667, mean_q: 6.665127\n",
      " 1381/5000: episode: 134, duration: 0.629s, episode steps: 38, steps per second: 60, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.097 [-1.271, 1.628], loss: 1.151930, mean_absolute_error: 3.605026, mean_q: 6.659391\n",
      " 1393/5000: episode: 135, duration: 0.200s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.090 [-1.184, 1.687], loss: 1.728663, mean_absolute_error: 3.597384, mean_q: 6.614550\n",
      " 1405/5000: episode: 136, duration: 0.204s, episode steps: 12, steps per second: 59, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.104 [-1.018, 1.494], loss: 1.257082, mean_absolute_error: 3.698722, mean_q: 6.867605\n",
      " 1448/5000: episode: 137, duration: 0.713s, episode steps: 43, steps per second: 60, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.130 [-1.234, 1.446], loss: 1.378092, mean_absolute_error: 3.837671, mean_q: 7.085038\n",
      " 1499/5000: episode: 138, duration: 0.849s, episode steps: 51, steps per second: 60, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.157 [-0.758, 1.170], loss: 1.357945, mean_absolute_error: 3.942661, mean_q: 7.242507\n",
      " 1556/5000: episode: 139, duration: 0.952s, episode steps: 57, steps per second: 60, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.142 [-0.695, 0.913], loss: 1.342891, mean_absolute_error: 4.040418, mean_q: 7.453827\n",
      " 1613/5000: episode: 140, duration: 0.948s, episode steps: 57, steps per second: 60, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.138 [-0.755, 0.596], loss: 1.281920, mean_absolute_error: 4.166401, mean_q: 7.716024\n",
      " 1664/5000: episode: 141, duration: 0.850s, episode steps: 51, steps per second: 60, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.549 [0.000, 1.000], mean observation: 0.168 [-1.037, 1.166], loss: 1.532334, mean_absolute_error: 4.373472, mean_q: 8.048825\n",
      " 1720/5000: episode: 142, duration: 0.934s, episode steps: 56, steps per second: 60, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.124 [-0.847, 0.527], loss: 1.676899, mean_absolute_error: 4.499464, mean_q: 8.296186\n",
      " 1762/5000: episode: 143, duration: 0.699s, episode steps: 42, steps per second: 60, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.189 [-0.346, 0.773], loss: 1.496194, mean_absolute_error: 4.584424, mean_q: 8.439153\n",
      " 1849/5000: episode: 144, duration: 1.453s, episode steps: 87, steps per second: 60, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.133 [-0.400, 0.889], loss: 1.742324, mean_absolute_error: 4.746913, mean_q: 8.733888\n",
      " 1946/5000: episode: 145, duration: 1.618s, episode steps: 97, steps per second: 60, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.116 [-0.542, 0.743], loss: 1.832685, mean_absolute_error: 5.014148, mean_q: 9.295918\n",
      " 2011/5000: episode: 146, duration: 1.078s, episode steps: 65, steps per second: 60, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.143 [-0.831, 0.470], loss: 1.633375, mean_absolute_error: 5.151834, mean_q: 9.574051\n",
      " 2077/5000: episode: 147, duration: 1.106s, episode steps: 66, steps per second: 60, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.094 [-0.856, 0.296], loss: 1.763347, mean_absolute_error: 5.373026, mean_q: 9.994999\n",
      " 2171/5000: episode: 148, duration: 1.562s, episode steps: 94, steps per second: 60, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.137 [-0.566, 0.942], loss: 2.390026, mean_absolute_error: 5.559367, mean_q: 10.222165\n",
      " 2243/5000: episode: 149, duration: 1.199s, episode steps: 72, steps per second: 60, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.109 [-0.728, 0.834], loss: 1.649949, mean_absolute_error: 5.702035, mean_q: 10.666514\n",
      " 2286/5000: episode: 150, duration: 0.716s, episode steps: 43, steps per second: 60, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.160 [-0.919, 0.576], loss: 1.468791, mean_absolute_error: 5.756581, mean_q: 10.879493\n",
      " 2335/5000: episode: 151, duration: 0.816s, episode steps: 49, steps per second: 60, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.151 [-0.912, 0.588], loss: 1.981286, mean_absolute_error: 5.980776, mean_q: 11.294813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2375/5000: episode: 152, duration: 0.667s, episode steps: 40, steps per second: 60, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.168 [-1.136, 0.733], loss: 2.861567, mean_absolute_error: 6.101355, mean_q: 11.344931\n",
      " 2423/5000: episode: 153, duration: 0.799s, episode steps: 48, steps per second: 60, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.140 [-0.906, 0.659], loss: 2.120062, mean_absolute_error: 6.239829, mean_q: 11.729821\n",
      " 2471/5000: episode: 154, duration: 0.803s, episode steps: 48, steps per second: 60, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.152 [-0.901, 0.562], loss: 2.615871, mean_absolute_error: 6.364263, mean_q: 11.911275\n",
      " 2523/5000: episode: 155, duration: 0.859s, episode steps: 52, steps per second: 61, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.127 [-0.739, 0.408], loss: 2.663216, mean_absolute_error: 6.495188, mean_q: 12.161321\n",
      " 2565/5000: episode: 156, duration: 0.702s, episode steps: 42, steps per second: 60, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.168 [-0.751, 0.432], loss: 2.359074, mean_absolute_error: 6.651750, mean_q: 12.516871\n",
      " 2613/5000: episode: 157, duration: 0.802s, episode steps: 48, steps per second: 60, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.138 [-1.354, 0.951], loss: 2.500366, mean_absolute_error: 6.703843, mean_q: 12.536316\n",
      " 2662/5000: episode: 158, duration: 0.813s, episode steps: 49, steps per second: 60, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.146 [-0.919, 0.399], loss: 2.360337, mean_absolute_error: 6.789190, mean_q: 12.803468\n",
      " 2722/5000: episode: 159, duration: 1.001s, episode steps: 60, steps per second: 60, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.103 [-0.809, 0.612], loss: 2.797504, mean_absolute_error: 6.955539, mean_q: 13.044421\n",
      " 2757/5000: episode: 160, duration: 0.580s, episode steps: 35, steps per second: 60, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.140 [-0.776, 0.447], loss: 1.988169, mean_absolute_error: 6.951547, mean_q: 13.202002\n",
      " 2813/5000: episode: 161, duration: 0.939s, episode steps: 56, steps per second: 60, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.095 [-0.803, 0.574], loss: 2.792663, mean_absolute_error: 7.189723, mean_q: 13.585466\n",
      " 2893/5000: episode: 162, duration: 1.332s, episode steps: 80, steps per second: 60, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.081 [-0.812, 0.605], loss: 2.289753, mean_absolute_error: 7.341927, mean_q: 13.977083\n",
      " 2933/5000: episode: 163, duration: 0.666s, episode steps: 40, steps per second: 60, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.122 [-0.715, 0.355], loss: 3.275460, mean_absolute_error: 7.515021, mean_q: 14.022636\n",
      " 2994/5000: episode: 164, duration: 1.012s, episode steps: 61, steps per second: 60, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.080 [-0.835, 0.607], loss: 2.064508, mean_absolute_error: 7.623799, mean_q: 14.592760\n",
      " 3029/5000: episode: 165, duration: 0.580s, episode steps: 35, steps per second: 60, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.134 [-0.866, 0.559], loss: 2.430893, mean_absolute_error: 7.620103, mean_q: 14.472978\n",
      " 3082/5000: episode: 166, duration: 0.888s, episode steps: 53, steps per second: 60, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.110 [-0.853, 0.535], loss: 2.823417, mean_absolute_error: 7.715183, mean_q: 14.619003\n",
      " 3123/5000: episode: 167, duration: 0.685s, episode steps: 41, steps per second: 60, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.120 [-0.897, 0.598], loss: 2.548575, mean_absolute_error: 7.789209, mean_q: 14.819901\n",
      " 3161/5000: episode: 168, duration: 0.629s, episode steps: 38, steps per second: 60, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.106 [-0.796, 0.434], loss: 2.349595, mean_absolute_error: 7.898644, mean_q: 15.135592\n",
      " 3183/5000: episode: 169, duration: 0.362s, episode steps: 22, steps per second: 61, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.111 [-0.724, 0.388], loss: 3.226286, mean_absolute_error: 8.064505, mean_q: 15.328547\n",
      " 3212/5000: episode: 170, duration: 0.487s, episode steps: 29, steps per second: 59, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.107 [-0.904, 0.607], loss: 4.089571, mean_absolute_error: 8.111373, mean_q: 15.270002\n",
      " 3245/5000: episode: 171, duration: 0.546s, episode steps: 33, steps per second: 60, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.119 [-0.893, 0.549], loss: 3.127234, mean_absolute_error: 8.077125, mean_q: 15.296376\n",
      " 3296/5000: episode: 172, duration: 0.849s, episode steps: 51, steps per second: 60, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.098 [-0.758, 0.428], loss: 2.921599, mean_absolute_error: 8.164300, mean_q: 15.562567\n",
      " 3323/5000: episode: 173, duration: 0.450s, episode steps: 27, steps per second: 60, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.112 [-0.662, 0.437], loss: 3.276498, mean_absolute_error: 8.290751, mean_q: 15.778260\n",
      " 3357/5000: episode: 174, duration: 0.566s, episode steps: 34, steps per second: 60, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.114 [-1.143, 0.729], loss: 2.891174, mean_absolute_error: 8.319614, mean_q: 15.909324\n",
      " 3400/5000: episode: 175, duration: 0.713s, episode steps: 43, steps per second: 60, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.087 [-0.824, 0.609], loss: 2.949769, mean_absolute_error: 8.424070, mean_q: 16.102190\n",
      " 3422/5000: episode: 176, duration: 0.374s, episode steps: 22, steps per second: 59, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.132 [-0.948, 0.565], loss: 4.058921, mean_absolute_error: 8.545264, mean_q: 16.201838\n",
      " 3457/5000: episode: 177, duration: 0.579s, episode steps: 35, steps per second: 60, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.111 [-0.619, 0.409], loss: 3.225437, mean_absolute_error: 8.580070, mean_q: 16.370518\n",
      " 3496/5000: episode: 178, duration: 0.648s, episode steps: 39, steps per second: 60, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.104 [-1.120, 0.765], loss: 2.305263, mean_absolute_error: 8.538556, mean_q: 16.452404\n",
      " 3537/5000: episode: 179, duration: 0.682s, episode steps: 41, steps per second: 60, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.104 [-0.978, 0.582], loss: 2.508098, mean_absolute_error: 8.638219, mean_q: 16.632385\n",
      " 3568/5000: episode: 180, duration: 0.521s, episode steps: 31, steps per second: 60, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.124 [-0.658, 0.216], loss: 3.320712, mean_absolute_error: 8.763333, mean_q: 16.753790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3605/5000: episode: 181, duration: 0.613s, episode steps: 37, steps per second: 60, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.112 [-0.659, 0.420], loss: 3.488212, mean_absolute_error: 8.999782, mean_q: 17.196478\n",
      " 3632/5000: episode: 182, duration: 0.447s, episode steps: 27, steps per second: 60, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.112 [-0.690, 0.357], loss: 4.030760, mean_absolute_error: 9.083974, mean_q: 17.324644\n",
      " 3657/5000: episode: 183, duration: 0.418s, episode steps: 25, steps per second: 60, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.112 [-0.633, 0.375], loss: 3.510044, mean_absolute_error: 9.089281, mean_q: 17.313381\n",
      " 3686/5000: episode: 184, duration: 0.476s, episode steps: 29, steps per second: 61, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.139 [-0.752, 0.347], loss: 3.025756, mean_absolute_error: 8.948306, mean_q: 17.184053\n",
      " 3756/5000: episode: 185, duration: 1.171s, episode steps: 70, steps per second: 60, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.064 [-0.775, 0.662], loss: 3.490980, mean_absolute_error: 9.094899, mean_q: 17.382601\n",
      " 3791/5000: episode: 186, duration: 0.581s, episode steps: 35, steps per second: 60, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.092 [-0.666, 0.391], loss: 3.360433, mean_absolute_error: 9.179161, mean_q: 17.675514\n",
      " 3810/5000: episode: 187, duration: 0.317s, episode steps: 19, steps per second: 60, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.117 [-0.958, 0.595], loss: 4.182384, mean_absolute_error: 9.373688, mean_q: 17.934469\n",
      " 3841/5000: episode: 188, duration: 0.514s, episode steps: 31, steps per second: 60, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.100 [-0.669, 0.353], loss: 3.198344, mean_absolute_error: 9.400779, mean_q: 18.133366\n",
      " 3896/5000: episode: 189, duration: 0.918s, episode steps: 55, steps per second: 60, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.075 [-0.795, 0.370], loss: 2.083674, mean_absolute_error: 9.344231, mean_q: 18.167906\n",
      " 3918/5000: episode: 190, duration: 0.370s, episode steps: 22, steps per second: 59, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.103 [-0.909, 0.609], loss: 4.947733, mean_absolute_error: 9.546087, mean_q: 18.278776\n",
      " 3945/5000: episode: 191, duration: 0.451s, episode steps: 27, steps per second: 60, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.089 [-0.868, 0.586], loss: 3.296103, mean_absolute_error: 9.668311, mean_q: 18.661867\n",
      " 3966/5000: episode: 192, duration: 0.343s, episode steps: 21, steps per second: 61, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.093 [-1.014, 0.601], loss: 4.950969, mean_absolute_error: 9.757344, mean_q: 18.671001\n",
      " 3991/5000: episode: 193, duration: 0.419s, episode steps: 25, steps per second: 60, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.117 [-0.664, 0.376], loss: 2.691027, mean_absolute_error: 9.697884, mean_q: 18.791372\n",
      " 4015/5000: episode: 194, duration: 0.399s, episode steps: 24, steps per second: 60, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.110 [-0.706, 0.372], loss: 2.748619, mean_absolute_error: 9.798051, mean_q: 19.045282\n",
      " 4040/5000: episode: 195, duration: 0.414s, episode steps: 25, steps per second: 60, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.104 [-0.685, 0.441], loss: 2.120366, mean_absolute_error: 9.723534, mean_q: 19.015253\n",
      " 4075/5000: episode: 196, duration: 0.583s, episode steps: 35, steps per second: 60, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.084 [-1.115, 0.825], loss: 4.983537, mean_absolute_error: 9.972201, mean_q: 19.162428\n",
      " 4110/5000: episode: 197, duration: 0.583s, episode steps: 35, steps per second: 60, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.086 [-0.831, 0.378], loss: 4.103903, mean_absolute_error: 10.036700, mean_q: 19.258963\n",
      " 4145/5000: episode: 198, duration: 0.583s, episode steps: 35, steps per second: 60, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.108 [-0.861, 0.557], loss: 4.517432, mean_absolute_error: 10.005996, mean_q: 19.207506\n",
      " 4187/5000: episode: 199, duration: 0.700s, episode steps: 42, steps per second: 60, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.087 [-0.884, 0.612], loss: 3.812157, mean_absolute_error: 10.107804, mean_q: 19.504900\n",
      " 4219/5000: episode: 200, duration: 0.533s, episode steps: 32, steps per second: 60, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.103 [-0.663, 0.361], loss: 4.692956, mean_absolute_error: 10.054873, mean_q: 19.268246\n",
      " 4264/5000: episode: 201, duration: 0.750s, episode steps: 45, steps per second: 60, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.077 [-0.593, 0.420], loss: 4.032429, mean_absolute_error: 10.283120, mean_q: 19.808714\n",
      " 4303/5000: episode: 202, duration: 0.650s, episode steps: 39, steps per second: 60, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.104 [-0.697, 0.343], loss: 5.411214, mean_absolute_error: 10.356687, mean_q: 19.808931\n",
      " 4333/5000: episode: 203, duration: 0.495s, episode steps: 30, steps per second: 61, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.072 [-1.168, 0.410], loss: 3.374016, mean_absolute_error: 10.273877, mean_q: 19.856546\n",
      " 4372/5000: episode: 204, duration: 0.650s, episode steps: 39, steps per second: 60, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.074 [-0.704, 0.438], loss: 5.647333, mean_absolute_error: 10.403688, mean_q: 19.823017\n",
      " 4404/5000: episode: 205, duration: 0.535s, episode steps: 32, steps per second: 60, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.078 [-0.811, 0.561], loss: 4.419557, mean_absolute_error: 10.482601, mean_q: 20.193867\n",
      " 4451/5000: episode: 206, duration: 0.786s, episode steps: 47, steps per second: 60, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.077 [-0.657, 0.439], loss: 3.898562, mean_absolute_error: 10.492323, mean_q: 20.236944\n",
      " 4494/5000: episode: 207, duration: 0.712s, episode steps: 43, steps per second: 60, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.080 [-0.875, 0.369], loss: 3.421651, mean_absolute_error: 10.507961, mean_q: 20.348204\n",
      " 4520/5000: episode: 208, duration: 0.433s, episode steps: 26, steps per second: 60, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.088 [-1.009, 0.384], loss: 4.477552, mean_absolute_error: 10.691323, mean_q: 20.614908\n",
      " 4560/5000: episode: 209, duration: 0.669s, episode steps: 40, steps per second: 60, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.089 [-0.829, 0.246], loss: 3.778114, mean_absolute_error: 10.709770, mean_q: 20.715185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4595/5000: episode: 210, duration: 0.581s, episode steps: 35, steps per second: 60, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.078 [-0.962, 0.343], loss: 4.577616, mean_absolute_error: 10.823954, mean_q: 20.768909\n",
      " 4632/5000: episode: 211, duration: 0.620s, episode steps: 37, steps per second: 60, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.124 [-0.627, 0.355], loss: 4.812831, mean_absolute_error: 10.816339, mean_q: 20.779085\n",
      " 4673/5000: episode: 212, duration: 0.683s, episode steps: 41, steps per second: 60, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.101 [-0.737, 0.348], loss: 4.687599, mean_absolute_error: 10.842155, mean_q: 20.817657\n",
      " 4706/5000: episode: 213, duration: 0.549s, episode steps: 33, steps per second: 60, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.084 [-0.634, 0.423], loss: 4.485213, mean_absolute_error: 10.893541, mean_q: 21.030254\n",
      " 4740/5000: episode: 214, duration: 0.564s, episode steps: 34, steps per second: 60, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.073 [-0.805, 0.405], loss: 4.601380, mean_absolute_error: 10.978286, mean_q: 21.210215\n",
      " 4768/5000: episode: 215, duration: 0.466s, episode steps: 28, steps per second: 60, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.094 [-0.672, 0.424], loss: 5.596440, mean_absolute_error: 10.985735, mean_q: 21.122587\n",
      " 4805/5000: episode: 216, duration: 0.618s, episode steps: 37, steps per second: 60, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.075 [-0.858, 0.585], loss: 5.029026, mean_absolute_error: 11.053699, mean_q: 21.267647\n",
      " 4833/5000: episode: 217, duration: 0.466s, episode steps: 28, steps per second: 60, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.099 [-0.690, 0.374], loss: 4.942134, mean_absolute_error: 11.092308, mean_q: 21.346296\n",
      " 4860/5000: episode: 218, duration: 0.453s, episode steps: 27, steps per second: 60, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.122 [-0.731, 0.351], loss: 4.762272, mean_absolute_error: 10.922999, mean_q: 20.967468\n",
      " 4886/5000: episode: 219, duration: 0.436s, episode steps: 26, steps per second: 60, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.112 [-0.671, 0.409], loss: 4.458753, mean_absolute_error: 11.098134, mean_q: 21.505787\n",
      " 4921/5000: episode: 220, duration: 0.584s, episode steps: 35, steps per second: 60, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.101 [-0.755, 0.397], loss: 5.700613, mean_absolute_error: 11.315138, mean_q: 21.744713\n",
      " 4960/5000: episode: 221, duration: 0.643s, episode steps: 39, steps per second: 61, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.102 [-0.794, 0.351], loss: 4.848750, mean_absolute_error: 11.090150, mean_q: 21.346905\n",
      " 4987/5000: episode: 222, duration: 0.455s, episode steps: 27, steps per second: 59, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.096 [-0.702, 0.382], loss: 3.652199, mean_absolute_error: 11.213796, mean_q: 21.759684\n",
      "done, took 84.126 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x208ca18bf28>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Ok, now it's time to learn something! We visualize the training here for show, but this slows down training quite a lot.\n",
    "dqn.fit(env, nb_steps=5000, visualize=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 81.000, steps: 81\n",
      "Episode 2: reward: 70.000, steps: 70\n",
      "Episode 3: reward: 79.000, steps: 79\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 200.000, steps: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2083faad588>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
